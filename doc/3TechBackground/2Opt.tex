Nonlinear optimisation, or nonlinear programming, is the process of attempting to minimise some cost function subject to a set of constraints, in which the one or more of the cost function and the constraints are nonlinear in the decision variables. 

While there are techniques which guarantee the convergence of an optimisation to the solution, if one exists, in linear and quadratic programming, there are no such guarantees for general nonlinear programming. Indeed, it is not even possible to reliably evaluate whether or not the optimisation problem is well-posed, i.e. whether a minimising solution is possible which satisfies the constraints. If an optimisation is well-posed, the main cause for the optimisation finding a solution which is not the true optimal is the existence of local minima, i.e. points which are the minimum in some region in the space of the decision variables but not over the entire space.

There are numerous methods for solving nonlinear programs, most of which attempt to find a local minimum near the initial estimate. These vary in implementation, but they are all by nature gradient-based methods. That is, the local minimum near the estimate is found by traversing down the slope at that point. There do exist methods which are not gradient-based \cite{perez2007particle, li2005multi}, however these are universally more computationally intensive.

The optimisation technique used in producing primitives, as described in Section \ref{sec:optmethod}, is gradient-based both in attempting to minimise the cost function and in the regulation of constraints; the so-called \textit{barrier function method}. The constraints are regulated by producing a function which penalises the proximity of the solution to the constraint. This barrier function is typically of the form $\phi(x)=-k\log(c(x))$, where $c(x)$ is zero at the constraint and positive otherwise. As the solution converges, the coefficient $k$ is reduced such that proximity to the boundary is made more permissive.