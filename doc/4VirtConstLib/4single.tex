\subsection{Motivation for optimisation approach}
Most approaches to designing Bézier polynomial-based virtual constraints for a bipedal robot can be classed in one of three categories; \textit{manual design}, \textit{sampling} or \textit{optimisation}. Manual design involves a human operator producing the control points directly. Sampling is the utilisation of some method of choosing coefficients in an attempt to span the configuration space, either by random selection or gridding. Optimisation approaches choose coefficients which minimise some cost function under particular constraints.

Manual design approaches are not favourable since they are expensive and inefficient in comparison to automatic generation of virtual constraints. Sampling methods, while much faster at generating a single virtual constraint than manual generation, suffer from the \textit{curse of dimensionality}. That is, in order for a sampling method to produce the same density of coverage, the number of samples required increases exponentially with the number of dimensions. Using a brute-force sampling approach produces many redundant primitives which exhibit similar net changes in the mechanical energy of the walker and paths of the end of the swing leg. It is also difficult to ensure that the conditions for feasible walking are met using pure sampling methods.

The redundancy inherent in the sampling approach increases the the memory required to store the virtual constraint library and the computational requirements of both its generation and use in real time. An optimisation approach attempts to find the best virtual constraint subject to particular requirements, e.g. beginning and final conditions. Therefore, the utility of single virtual constraint optimisation is to avoid the large amount of redundancy involved in the sampling approach and to enable the enforcement of feasible walking conditions. Clearly, single VC optimisation is itself not sufficient to produce a library of primitives; see Section \ref{sec:lib}.

\subsection{Definition of optimality}
As with most physical systems, there are competing definitions of optimality in the case walking robots; {\color{red}<insert literature references to competing definitions>}.

The definition of optimality chosen for the purposes of producing the virtual constraint library is as follows:
\emph{The optimal virtual constraint for a given start and end configuration and kinetic energy gain or loss is the one which requires the minimum input energy to maintain.}

We note that the relationship between torque and electrical energy in conventional DC motors is typically approximated by the following equation:
\begin{equation} \label{eqn:motorenergy}
	E(t) \approx \int\limits_0^t u(s)^2 ~ ds
\end{equation}

However, since the partial solution of the zero dynamics is in terms of $\theta$ rather than time, this is not a convenient cost function. Therefore, we assume that $\theta$ progresses reasonably steadily on the interval $[\theta_0,\theta^-]$ and thus the cost function
\begin{equation}
	J_\alpha = \int\limits_{\theta_0}^{\theta^-} \left\lVert u_\alpha(\theta) \right\rVert_2^2 ~ d\theta
\end{equation}
approximates Equation \ref{eqn:motorenergy} evaluated over the virtual constraint. Note that the L2-norm is included to encapsulate the contribution of multiple torques, since in all but the simplest walking robots, there is more than one actuated joint.

\subsection{Validity of convex optimisation approach}
The decision variables for the purpose of optimisation are the Bézier coefficients $\alpha$. Since the formulation of the cost function $J$ is not linear or quadratic in these decision variables, the optimisation problem becomes more complex; see Section \ref{sec:nonlinopt}. Linear and quadratic techniques are not immediately applicable to the nonlinear case, leaving two alternatives; gridding approaches and nonlinear programming. Gridding approaches in optimisation are subject to the same limitations described above; indeed, if the advantage of optimisation is to avoid using gridding, then such an approach is clearly a poor choice. However, nonlinear programming will only produce reasonable results under certain conditions.

It is not possible to determine \textit{a priori} that an optimisation is well-formed, that is, will result in a valid solution. Furthermore, for nonlinear optimisation to produce favourable results, the cost function must be convex or near-convex close to the initial estimate. It is therefore of significant interest to determine how close the cost function is to being convex. For the two-link compass gait walker, it is possible to produce such a verification, since we can formulate an optimisation with only two decision variables, which is easily visualised. In Figure \ref{fig:cggridtorque}, we see that gridding the two decision variables produces a surface that appears to be well-behaved and convex. It is reasonable to suggest that this is generalisable to higher degree constraints with more decision variables for the compass-gait robot.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{4VirtConstLib/CGgrid.eps}
	\caption{Cost function of a two decision variable compass-gait optimisation}
	\label{fig:cggridtorque}
\end{figure}

In the case of higher-DOF walking models, it becomes more difficult to confirm near-convexity or even that the cost function is well-behaved. Taking slices, where we choose two decision variables over which to grid and fix all others, allows us to develop some intuition, see Figure \ref{fig:5lslice1} and \ref{fig:5lslice2}. We note that in these slices, the cost function again appears to be well-behaved and near-convex. On the basis of this evidence, it appears to be reasonable to assume that the convex optimisation approach will produce valid near-optimal results.

\subsection{Optimisation method}
\subsubsection{Decision variables and constraints}
While the objective of single constraint optimisation is to produce an optimum VC independent of any other virtual constraints in the library, it is important to formulate the optimisation problem in a manner which enables a useful library to be produced. The motion primitive library is intended to contain VCs which span the configuration space of the robot's natural walking as well as including a range of mechanical energy additions and subtractions to facilitate walking over uneven terrain. Therefore, it seems clear that each single constraint optimisation should be formulated subject to constraints on the start and end configurations of the footstep, along with a prescribed energy gain or loss. In addition, it is necessary to specify the ground height applicable to the virtual constraint, since a feasible step cannot intersect with the ground between the start and end conditions.

Since the potential energy change is prescribed by the start and end conditions, it is convenient to formulate the optimisation using kinetic, rather than total mechanical, energy. The change in kinetic energy $\Delta$KE from one footstep to the next could be calculated with respect to numerous reference points. A simple and obvious choice is to define $\Delta$KE as the change in kinetic energy from $\theta_0 \rightarrow \theta^+$.

The ground height at a horizontal displacement $x$ from the end of the stance leg is encoded in the scalar function $\sigma(x)$. For the purposes of analysing a virtual constraint, the ground height is pertinent only when considering the end of the swing leg, thus we may produce a more convenient representation $\sigma(\theta)=\sigma(p_h(\theta))$. The ground height places the following nonlinear constraints on the optimisation:
\begin{equation} \label{eqn:groundheight}
	p_v(\theta) \left\{
	\begin{array}{lcr}
		= \sigma(\theta) &:& \theta \in \{\theta_0, \theta^-\} \\
		> \sigma(\theta) &~& \mathrm{otherwise}
	\end{array} \right.
\end{equation}

Recall from Section \ref{sec:bezconstraints} that in order to be physically realisable, a VC must satisfy several conditions. The \textit{invariance conditions}, \ref{item:configinvariance} and \ref{item:velinvariance}, place restrictions on the admissibility of a primitive on the basis of the VC which precedes it. The first condition is not of great concern in the construction of a single primitive, since the responsibility of ensuring coverage of start and final configurations is deferred to the library generation. However, it is important to consider the latter condition when generating a single constraint; in order for the library to be useful, any VC which matches the prior primitive through the impact map should be a valid choice of successor. This implies that the slope of all constraints at the end point for any given final configuration ought to be identical. A reasonable choice to satisfy this condition is to enforce that the final slope must be zero for all constraints. This has the additional advantage of making the constraints more robust to errors in ground height perception.

When formulating an optimisation, it is advantageous to minimise the number of decision variables, if all else is considered static, since this reduces the evaluation time. This is particularly true for variables which are subject to constraints. It is notable that fixing the start and end configuration of the constraint along with enforcing zero slope at the final configuration fixes the four outermost columns of $\alpha$. As such, we consider these fixed and exclude them as decision variables. The optimisation variables therefore take the form given in Table \ref{tab:optDecVars}. Note that the conditions in Section \ref{sec:bezconstraints} along with the equality constraint in Equation \ref{eqn:groundheight} are all concerned with those four outermost columns of $\alpha$, therefore in the single VC optimisation, these conditions are not applicable.

\begin{table}
	\centering
	\begin{tabular}{c | c | c | c}
		            Variable             & Generated                     & Decision Variable & Opt constraint \\ \hline
		   $\theta_0$ and $\theta^-$     & From start/end condition      & No                & No             \\
		   $\alpha_0$ and $\alpha_N$     & From start/end condition      & No                & No             \\
		         $\alpha_{N-1}$          & Set to $\alpha_N$             & No                & No             \\
		           $\alpha_1$            & From \ref{item:velinvariance} & No                & No             \\
		$\alpha_2, \ldots, \alpha_{N-2}$ & Optimisation                  & \textbf{Yes}      & No             \\
		           $\Delta$KE            & Supplied                      & No                & \textbf{Yes}   \\
		          $\sigma(x)$            & Supplied                      & No                & \textbf{Yes}
	\end{tabular}
	\caption{Variables involved in the single VC optimisation}
	\label{tab:optDecVars}
\end{table}

\subsubsection{Nominal initial velocity}
The torque required to maintain a virtual constraint is not only dependent on the configuration path, but also the velocity. VCs prescribe the configuration path and constrain the velocity to be a function of the initial phase variable velocity. As a result, the torque required to maintain a virtual constraint is a function of the initial velocity. In addition, $\Delta$KE is affine in the square of the initial phase variable velocity. It is therefore necessary to formulate a \textit{nominal initial velocity}, $\dot{\theta}_\alpha^*$. We assume that in practice, the constraint will be chosen with initial velocities close to $\dot{\theta}_\alpha^*$ and should therefore be near-optimal. Under that assumption, the change in kinetic energy should be close to the prescribed $\Delta$KE, however this is less important since setting $\Delta$KE is in service of producing a rich library.

Choosing a nominal velocity for each constraint is non-trivial. It should be noted that the choice of nominal velocity significantly influences the optimisation. Choosing a single $\dot{\theta}^*$ for all VCs is not appropriate, each virtual constraint may differ in the minimum velocity required to complete the motion. It seems necessary to treat the constraints separately based upon the desired $\Delta$KE and minimum initial velocity $\dot{\theta}^c_\alpha$.

For \textbf{energy-additive constraints}, for which $\Delta$KE $>0$, the key issue of importance is that the virtual constraint itself is feasible, since the next primitive will have additional kinetic energy. Therefore, we set the initial velocity such that $\dot{\theta}_\alpha^* = \lambda\dot{\theta}_\alpha^c$, where $\lambda$ is some fixed ``factor of safety'':
\begin{equation} \label{eqn:energyadditive}
\dot{\theta}_{\alpha,\Delta\mathrm{KE}^+}^{*^2} = -\lambda^2\frac{ \Psi_\alpha(\theta_\alpha^c)}{ \Gamma_\alpha(\theta_\alpha^c)}
\end{equation}

For \textbf{energy-subtractive constraints}, for which $\Delta$KE $<0$, assuming that the robot should never come to a stop or fall back, the energy reduction should result in further constraints being feasible. In order to avoid using any data external to the constraint, we use the value of $\dot{\theta}_\alpha^c$ associated with the same VC. This is reasonable, particularly if the constraint is operating over terrain which has a systematic structure over greater distances than the step length, e.g. a long downward slope. The post impact velocity is calculable with $\Delta_\alpha$ as defined in Equation \ref{eqn:Delthd}:
\begin{subequations}
	\begin{align}
	\dot{\theta}^{-^2} &= \Gamma_\alpha(\theta_\alpha^-)\dot{\theta}_\alpha^{*^2} + \Psi_\alpha(\theta_\alpha^-) \\
	\dot{\theta}^+ &= c\Delta_\alpha\dot{\theta}^-
	\end{align}
\end{subequations}
Equating $\dot{\theta}^+$ with $\lambda\dot{\theta}_\alpha^c$, where $\lambda$ is the same factor of safety as in Equation \ref{eqn:energyadditive}:
\begin{equation}
	\dot{\theta}_{\alpha,\Delta\mathrm{KE}^-}^{*^2} = \frac{(c\Delta_\alpha)^{-2}\left(-\lambda^2\frac{ \Psi(\theta_\alpha^c)}{ \Gamma(\theta_\alpha^c)}\right) - \Psi(\theta_\alpha^-)} {\Gamma(\theta_\alpha^-)}
\end{equation}

\textbf{Energy-neutral constraints} result in $\Delta$KE $=0$. In this case, we expect the next constraint to require an initial $\dot{\theta}$ no greater or less than $\dot{\theta}_\alpha^*$. As such, either of the above expressions seem suitable. The simplicity of Equation \ref{eqn:energyadditive} is preferred.

\subsection{Implementation}
The \mcode{optimiseConstraint} function takes as arguments the start and end configurations of the constraint, the desired change in kinetic energy, the ground definition $\sigma(x)$ and the degree of the desired Bézier polynomial. It returns the start and end values of the phase variable $\theta_0$, $\theta^-$ and the Bézier coefficients $\alpha$. These are sufficient to define the virtual holonomic constraint, however, for convenience, it also returns the calculated values of the partial solution at $\theta_c$, $\theta^-$ and $\theta^+$.

The single VC implementation is achieved using MATLAB's constrained nonlinear programming function \mcode{fmincon}. This uses an interior point method to seek local minima in the cost function subject to the given constraints. For each set of test values for the decision variables, the partial solution ($\Gamma(\theta)$ and $\Psi(\theta)$) is evaluated by producing a grid of $\theta$ values in $[\theta_0, \theta^-]$. The cost is calculated using trapezoidal integration on the resulting torque values. Since $\Delta$KE also relies upon the partial solution, it is held persistently in a helper function and only recalculated upon differing decision variables, to avoid inefficient recalculation.

$\sigma(x)$ is defined by specifying the ground height at individual points. From this, the ground level over all $x$ is determined by a zero-order hold. The ground constraint is enforced by taking relatively sparse samples from $\theta_0$ to $\theta^-$ and testing for each sample that $p_v(\theta)>\sigma(\theta)$. A natural choice of samples is at each Bézier control point, such that the sampling of the ground is dependent on the degree of the polynomial. It is reasonable to test only several points along the trajectory on the assumption that the ground height changes at relatively few points over one footstep and that the swing leg traces a smooth arc between its start and end points. The second assumption is valid for all physical robots. The first is able to be justified by the design of the library generation.

The optimisation is not subjected to any additional bounds other than the $\Delta$KE and $\sigma(x)$ constraints. This is due to the well-behaved nature of the problem; large deviations always cost more in terms of input torque than small ones. It therefore makes little sense to arbitrarily constrain the decision variables. In the case of the compass-gait robot, due to its morphological simplicity, the $\sigma(x)$ constraint is not applied. Indeed, it is not possible for the compass-gait robot to complete walking motion on flat ground without scuffing its swing leg if no means are provided to shorten it.

\subsubsection{Accuracy of numerical solutions}
As discussed above, the optimisation of a virtual constraint is significantly dependent upon the numerical partial solution for velocity and energy. The accuracy of the numerical solution therefore limits the optimality of the constraint and the accuracy to which the prescribed $\Delta$KE is achieved. This is not of great significance, since each VC is optimised only for a single initial velocity and the prescription of $\Delta$KE is performed in order to produce a diverse library, which is attained regardless. However, of much greater concern is the fidelity of the data produced by the optimisation.

The partial solution (the functions $\Gamma(\theta)$ and $\Psi(\theta)$) is produced by constructing a grid of $\theta$ values, over which functions are evaluated and integrated, see Appendix \ref{sec:numint}. The critical issue here is that because the partial solution is generated by \textit{integration} over a grid, the accuracy of the solution is particularly sensitive to the grid density. To understand this, consider that numerically sampling a function produces the precise value of that function for the particular chosen values, but integrating sampled functions requires approximation of the shape between sampled points and therefore grows in error from the start to the end point of the integration, as shown in Figure \ref{fig:samplingerror}.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{4VirtConstLib/samplingerror.eps}
	\caption{Sampling error in $\Gamma(\theta)$ and $\Psi(\theta)$}
	\label{fig:samplingerror}
\end{figure}

The extent of the loss of accuracy due to sampling was evaluated by taking optimising a self-invariant constraint on flat ground with the target $\Delta$KE $=0$ and testing the proximity of $\theta^+$ to $\theta^*$. The results are presented in Table \ref{tab:samplingerror}.

\begin{table}
	\centering
	\begin{tabular}{ c | c | c | c }
		Number of samples & $\dot{\theta}^*$ (rad/s) & $\dot{\theta}^+$ (rad/s) & Error \\ \hline
		10 & 1.34 & 1.25 & \\
		25 & 1.42 & 1.32 & \\
		50 & 1.42 & 1.32 & \\
		200 & 1.43 & 1.34 &
	\end{tabular}
	\caption{Error from sampling in numerical integration.}
	\label{tab:samplingerror}
	{\color{red} It would appear something else is at play here.}
\end{table}